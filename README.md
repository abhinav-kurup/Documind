# ğŸ“„ DocuMind AI

<div align="center">

**Intelligent Document Analysis Platform**

*Chat with your PDFs using Local LLMs powered by Ollama*

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-green.svg)](https://www.langchain.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

[Features](#-features) â€¢ [Demo](#-demo) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Tech Stack](#-tech-stack)

</div>

---

## ğŸ¯ Overview

DocuMind AI is a powerful document analysis platform that enables you to have intelligent conversations with your PDF documents. Built on a multi-agent architecture with LangGraph, it provides accurate, citation-backed answers using local LLM inference via Ollama.

**ğŸ”’ Privacy-First:** All processing happens locally - no data leaves your machine!

---

## âœ¨ Features

### ğŸ¤– **Intelligent Query Routing**
- Automatically classifies queries as document-related or conversational
- Prevents unnecessary processing for out-of-scope questions
- LLM-based classification with keyword fallback for robustness

### ğŸ“š **Multi-Document Support**
- Upload and process multiple PDF files simultaneously
- Automatic text extraction and intelligent chunking
- Persistent vector storage with ChromaDB

### ğŸ’¬ **Conversational Interface**
- Clean, modern chat UI built with Streamlit
- Real-time processing indicators
- Message history with context preservation

### ğŸ” **Advanced Retrieval**
- Semantic similarity search using vector embeddings
- Top-k document retrieval (configurable)
- Context-aware response generation

### ğŸ“Š **Source Citations**
- Every answer includes document references
- Page numbers for easy verification
- Expandable source preview with document excerpts

### ğŸ”„ **Multi-Agent Workflow**
- **Router Agent:** Query classification
- **Retrieval Agent:** Semantic document search
- **Extraction Agent:** Structured data extraction
- **Analysis Agent:** Response synthesis with citations

### ğŸ“ˆ **Audit & Monitoring**
- Complete query execution trail
- Step-by-step agent logging
- JSONL format for easy analysis

### ğŸ›ï¸ **System Controls**
- Reset system state
- Clear vector database
- Real-time model configuration

---

## ğŸ–¼ï¸ Demo

### Main Interface
![DocuMind AI Interface](assets/screenshot_main_interface.png)
*Ask questions and get instant answers with citations from your documents*

### Intelligent Query Routing
![Query Routing](assets/screenshot_query_routing.png)
*Smart routing detects conversational queries and guides users to ask document-related questions*

### Document Processing
![Document Processing](assets/screenshot_document_processing.png)
*Easy drag-and-drop PDF upload with real-time processing feedback*

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+**
- **Ollama** installed and running ([Download](https://ollama.ai))
- 8GB+ RAM (16GB recommended)
- GPU optional (recommended for faster inference)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/Documind.git
cd Documind
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Set up Ollama**
```bash
# Pull the default model
ollama pull qwen2.5:3b

# Verify Ollama is running
ollama list
```

4. **Configure environment** (optional)
```bash
cp .env.example .env
# Edit .env with your settings
```

5. **Run the application**
```bash
streamlit run app.py
```

6. **Open in browser**
```
http://localhost:8501
```

---

## ğŸ“– Usage

### Uploading Documents

1. Click **"Upload PDFs"** in the sidebar
2. Select one or more PDF files
3. Click **"Process Documents"**
4. Wait for embedding completion âœ…

### Asking Questions

**Document-Related Queries:**
```
âœ… "What is the revenue mentioned in the report?"
âœ… "Summarize the key findings on page 5"
âœ… "Extract employee distribution data"
âœ… "What percentage increase is shown?"
```

**System Features:**
- Answers include **source citations** with page numbers
- Click **"ğŸ“‹ View Sources & Reasoning"** to see retrieved documents
- Check **"ğŸ“Š Audit Logs"** tab for execution details

### Managing Your Knowledge Base

- **Reset System:** Clear memory and reload components
- **Clear Database:** Delete all documents from vector store
- **Model:** Displays currently active LLM model

---

## ğŸ› ï¸ Tech Stack

### Core Technologies

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **LLM Framework** | LangChain | Orchestration & chains |
| **Workflow Engine** | LangGraph | Multi-agent state machine |
| **LLM Backend** | Ollama | Local LLM inference |
| **Vector Database** | ChromaDB | Embedding storage & search |
| **Embeddings** | sentence-transformers | all-MiniLM-L6-v2 model |
| **PDF Processing** | PyMuPDF | Text extraction |
| **UI Framework** | Streamlit | Web interface |
| **Config Management** | python-dotenv | Environment variables |

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Streamlit UI Layer                â”‚
â”‚  (Chat, Document Upload, Audit Viewer)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LangGraph Orchestrator              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Router  â”‚â†’ â”‚Retrieval â”‚â†’ â”‚ Analysis   â”‚ â”‚
â”‚  â”‚ Agent   â”‚  â”‚  Agent   â”‚  â”‚   Agent    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚                                      â”‚
â”‚       â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚Rejectionâ”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Infrastructure Layer                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ChromaDB  â”‚  â”‚ Ollama  â”‚  â”‚  Audit    â”‚  â”‚
â”‚  â”‚(Vectors) â”‚  â”‚(LLM API)â”‚  â”‚  Logger   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file in the root directory:

```env
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
LLM_MODEL=qwen2.5:3b

# Vector Database
CHROMA_DB_DIR=data/chroma
```

### Model Selection

Supported Ollama models:
- `qwen2.5:3b` - **Recommended** (fast, good for CPU)
- `llama3` - Balanced performance
- `mistral` - Good accuracy
- `phi3` - Lightweight

To change models:
```bash
ollama pull <model-name>
# Update LLM_MODEL in .env
```

---

## ğŸ“ Project Structure

```
Documind/
â”œâ”€â”€ agents/                  # LangGraph agents
â”‚   â”œâ”€â”€ router.py           # Query classification
â”‚   â”œâ”€â”€ retrieval.py        # Vector search
â”‚   â”œâ”€â”€ extraction.py       # Data extraction
â”‚   â””â”€â”€ analysis.py         # Response generation
â”œâ”€â”€ audit/                  # Logging system
â”‚   â””â”€â”€ logger.py           # Query audit trails
â”œâ”€â”€ core/                   # Core modules
â”‚   â”œâ”€â”€ config.py           # Configuration
â”‚   â”œâ”€â”€ state.py            # State definitions
â”‚   â””â”€â”€ orchestrator.py     # Workflow orchestrator
â”œâ”€â”€ document_processing/    # PDF handling
â”‚   â”œâ”€â”€ loader.py           # PDF loading
â”‚   â””â”€â”€ chunking.py         # Text chunking
â”œâ”€â”€ vectorstore/            # Vector DB
â”‚   â””â”€â”€ chroma.py           # ChromaDB wrapper
â”œâ”€â”€ assets/                 # README images
â”œâ”€â”€ data/                   # Runtime data
â”‚   â”œâ”€â”€ chroma/            # Vector database
â”‚   â”œâ”€â”€ documents/         # Uploaded PDFs
â”‚   â””â”€â”€ logs/              # Audit logs
â”œâ”€â”€ app.py                 # Main Streamlit app
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md             # This file
```

---

## ğŸ”§ Advanced Features

### Progressive Audit Logging

Every agent step is logged in real-time to `data/logs/`:
- `audit_trail.jsonl` - Full query results
- `audit_trail_steps.jsonl` - Individual agent steps

Example log entry:
```json
{
  "query_id": "abc-123",
  "timestamp": "2025-12-23T02:00:00",
  "step": "RetrievalAgent",
  "status": "Success",
  "retrieved_count": 5
}
```

### Custom Chunk Size

Adjust in `document_processing/chunking.py`:
```python
DocumentChunker(
    chunk_size=2000,    # Characters per chunk
    chunk_overlap=200   # Overlap for context
)
```

### Retrieval Top-K

Modify in `agents/retrieval.py`:
```python
results = self.vector_store.similarity_search(query, k=5)
# Increase k for more context, decrease for faster search
```

---

## ğŸ› Troubleshooting

### Ollama Connection Error
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama
ollama serve
```

### ChromaDB Lock Error
```bash
# Stop Streamlit first, then:
rm -rf data/chroma/*
```

### Slow Performance
- Use smaller model: `qwen2.5:3b` instead of `llama3`
- Reduce chunk size in `chunking.py`
- Lower retrieval k value in `retrieval.py`

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [LangChain](https://www.langchain.com/) for the orchestration framework
- [Ollama](https://ollama.ai/) for local LLM inference
- [ChromaDB](https://www.trychroma.com/) for vector storage
- [Streamlit](https://streamlit.io/) for the UI framework

---

## ğŸ“ Contact

**Project Link:** [https://github.com/yourusername/Documind](https://github.com/yourusername/Documind)

---

<div align="center">

**Built with â¤ï¸ using Local LLMs**

*No API keys required â€¢ 100% Privacy â€¢ Fully Open Source*

</div>
